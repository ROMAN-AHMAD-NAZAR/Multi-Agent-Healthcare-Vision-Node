# ============================================================
# ğŸ¤– MEDICAL MULTI-AGENT ORCHESTRATOR
# ============================================================
# This script demonstrates how the Vision Agent Node integrates
# into a larger Multi-Agent System managed by an LLM Orchestrator.
#
# Architecture:
#   User Request â†’ Orchestrator â†’ Vision Agent â†’ JSON Response â†’ Report
# ============================================================

import json
import time
from datetime import datetime

# --- 1. MOCK VISION AGENT NODE (The "Plug" for your Notebook) ---
# In production, you would import this: 
# from Vision_Agent_2_5D_Attention import run_vision_agent

def call_vision_node(image_path):
    """
    Simulates calling the Vision Agent Node.
    In production, this would execute the actual model inference.
    """
    print(f"\n[System] ğŸ“¡ Connecting to Vision Agent Node...")
    print(f"[System] ğŸ“¤ Sending payload: {image_path}")
    
    # Simulate processing time (the 'thinking' phase)
    time.sleep(2) 
    
    # This is the exact JSON format your Notebook outputs
    vision_response = {
        "agent_id": "vision_expert_01",
        "agent_type": "2.5D_Attention_UNet",
        "timestamp": datetime.now().isoformat(),
        "diagnosis": "Glioma",
        "confidence_score": 0.94,
        "stability_check": "PASSED",
        "stability_score": 0.9812,
        "tumor_size_cm": 2.45,
        "tumor_location": "Temporal Lobe (Approximated)",
        "explanation_path": "./outputs/gradcam_heatmap.png",
        "model_version": "v1.0.0"
    }
    
    print(f"[System] ğŸ“¥ Received structured data from Vision Node.")
    return vision_response


# --- 2. MOCK VALIDATION AGENT (Neo4j Knowledge Graph) ---
def call_validation_agent(diagnosis):
    """
    Simulates the Validation Agent that cross-references
    diagnosis with medical knowledge graphs.
    """
    print(f"\n[System] ğŸ” Querying Validation Agent (Neo4j)...")
    time.sleep(1)
    
    # Simulated knowledge graph lookup
    knowledge_base = {
        "Glioma": {
            "severity": "High",
            "common_treatments": ["Surgery", "Radiation Therapy", "Chemotherapy"],
            "survival_rate": "Variable (depends on grade)",
            "icd_code": "C71.9"
        },
        "Meningioma": {
            "severity": "Low to Moderate",
            "common_treatments": ["Observation", "Surgery", "Radiation"],
            "survival_rate": "Generally favorable",
            "icd_code": "D32.9"
        },
        "No Tumor": {
            "severity": "None",
            "common_treatments": ["None required"],
            "survival_rate": "N/A",
            "icd_code": "N/A"
        }
    }
    
    return knowledge_base.get(diagnosis, {"severity": "Unknown"})


# --- 3. THE ORCHESTRATOR (The "Brain") ---
class MedicalOrchestrator:
    """
    Central orchestrator that manages the multi-agent workflow.
    In production, this would be powered by an LLM (GPT-4, Claude, etc.)
    """
    
    def __init__(self):
        self.system_prompt = """You are a helpful medical assistant. 
        Use specialized tools for precise diagnosis. 
        Always validate findings with knowledge graphs."""
        self.conversation_history = []
    
    def process_request(self, user_query, image_path=None):
        """
        Main entry point for processing user requests.
        Implements routing logic to determine which agents to invoke.
        """
        print("=" * 60)
        print(f"ğŸ”¹ USER: {user_query}")
        print("=" * 60)
        
        # LOGIC 1: ROUTING (Deciding what to do)
        if image_path:
            print("\n[Orchestrator] ğŸ§  Intent detected: Medical Image Analysis")
            print("[Orchestrator] ğŸ› ï¸  Routing to: VISION_AGENT_NODE")
            
            # Step 1: Call Vision Agent
            vision_data = call_vision_node(image_path)
            
            # Step 2: Validate with Knowledge Graph
            print("\n[Orchestrator] ğŸ› ï¸  Routing to: VALIDATION_AGENT")
            validation_data = call_validation_agent(vision_data["diagnosis"])
            
            # Step 3: Synthesize final response
            return self.synthesize_response(vision_data, validation_data)
        else:
            return "[Orchestrator] âš ï¸ Please provide an MRI image for analysis."

    def synthesize_response(self, vision_data, validation_data):
        """
        Combines outputs from multiple agents into a coherent report.
        This simulates how an LLM would generate natural language from structured data.
        """
        print("\n[Orchestrator] ğŸ“ Synthesizing Final Report...")
        time.sleep(1)
        
        # Build the clinical report
        confidence_pct = vision_data['confidence_score'] * 100
        stability_status = "âœ… Stable" if vision_data['stability_check'] == "PASSED" else "âš ï¸ Unstable"
        
        report = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          ğŸ¥ AUTOMATED PRELIMINARY DIAGNOSTIC REPORT          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Date: {vision_data['timestamp'][:19]}                          
â•‘  Agent: {vision_data['agent_id']} ({vision_data['model_version']})              
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                      PRIMARY FINDING                         â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Diagnosis: {vision_data['diagnosis'].upper()}
â•‘  Location:  {vision_data['tumor_location']}
â•‘  Size:      {vision_data['tumor_size_cm']} cm
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                      AI CONFIDENCE                           â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Confidence Score:  {confidence_pct:.1f}%
â•‘  Stability Check:   {stability_status}
â•‘  Stability Score:   {vision_data['stability_score']}
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                 KNOWLEDGE GRAPH VALIDATION                   â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Severity Level:    {validation_data['severity']}
â•‘  ICD-10 Code:       {validation_data.get('icd_code', 'N/A')}
â•‘  Common Treatments: {', '.join(validation_data.get('common_treatments', ['N/A']))}
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                      NEXT STEPS                              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  1. Review XAI Saliency Map: {vision_data['explanation_path']}
â•‘  2. Consult with specialist for confirmation
â•‘  3. Schedule follow-up imaging if required
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âš ï¸ DISCLAIMER: This is an AI-assisted preliminary analysis.
   Final diagnosis must be confirmed by a qualified physician.
"""
        return report


# --- 4. INTERACTIVE CLI DEMO ---
def run_interactive_demo():
    """
    Interactive demonstration mode for presenting to stakeholders.
    """
    print("\n" + "="*60)
    print("   ğŸ¤– MEDICAL MULTI-AGENT SYSTEM - INTERACTIVE DEMO")
    print("="*60)
    print("\nThis demo simulates how the Vision Agent integrates")
    print("into a larger healthcare AI pipeline.\n")
    
    orchestrator = MedicalOrchestrator()
    
    while True:
        print("\n" + "-"*40)
        print("OPTIONS:")
        print("  [1] Analyze sample MRI scan")
        print("  [2] View system architecture")
        print("  [3] Exit")
        print("-"*40)
        
        choice = input("Select option: ").strip()
        
        if choice == "1":
            user_query = "Can you analyze this brain MRI for any anomalies?"
            test_image = "./data/test/patient_scan_001.jpg"
            
            report = orchestrator.process_request(user_query, test_image)
            print(report)
            
        elif choice == "2":
            print("""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              MULTI-AGENT SYSTEM ARCHITECTURE            â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  User Input  â”‚
         â”‚  (MRI Scan)  â”‚
         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     ORCHESTRATOR      â”‚  â—„â”€â”€ LLM-Powered Decision Maker
    â”‚   (This Script)       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                 â”‚
       â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   VISION    â”‚   â”‚   VALIDATION    â”‚
â”‚   AGENT     â”‚   â”‚     AGENT       â”‚
â”‚ (Notebook)  â”‚   â”‚   (Neo4j KG)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                   â”‚
       â”‚    JSON Payloads  â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   CLINICAL   â”‚
         â”‚    REPORT    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            """)
            
        elif choice == "3":
            print("\nğŸ‘‹ Exiting Multi-Agent System Demo. Goodbye!")
            break
        else:
            print("âŒ Invalid option. Please select 1, 2, or 3.")


# --- 5. RUN THE SIMULATION ---
if __name__ == "__main__":
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                                                           â•‘
    â•‘   ğŸ§  MULTI-AGENT HEALTHCARE VISION SYSTEM                 â•‘
    â•‘                                                           â•‘
    â•‘   Demonstrating Integration of:                           â•‘
    â•‘   â€¢ Vision Agent Node (2.5D Attention U-Net)              â•‘
    â•‘   â€¢ Validation Agent (Knowledge Graph)                    â•‘
    â•‘   â€¢ LLM Orchestrator (Central Manager)                    â•‘
    â•‘                                                           â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # Run simple demo first
    print("--- RUNNING AUTOMATED DEMO ---\n")
    
    bot = MedicalOrchestrator()
    
    # Scenario: A doctor uploads a scan
    user_query = "Can you check this MRI for any anomalies?"
    test_image = "./data/test/scan_04.jpg"
    
    final_output = bot.process_request(user_query, test_image)
    print(final_output)
    
    # Ask if user wants interactive mode
    print("\n" + "="*60)
    response = input("Would you like to enter interactive mode? (y/n): ").strip().lower()
    if response == 'y':
        run_interactive_demo()
